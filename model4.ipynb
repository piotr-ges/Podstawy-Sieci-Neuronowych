{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-19T03:56:33.701382100Z",
     "start_time": "2025-01-19T03:56:33.682606400Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.models.detection import transform\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.nn.functional import max_pool2d\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import RandomChoice\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import seaborn as sns\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs_number = 200\n",
    "patience = 5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T03:56:33.702985500Z",
     "start_time": "2025-01-19T03:56:33.687147800Z"
    }
   },
   "id": "4e5f24e0aa04152"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "#Create dataset\n",
    "class PlayingCardDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data = ImageFolder(data_dir, transform=transform)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    @property\n",
    "    def classes(self):\n",
    "        return self.data.classes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T03:56:33.702985500Z",
     "start_time": "2025-01-19T03:56:33.691738700Z"
    }
   },
   "id": "4e2e6b6e8619f882"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "#Model\n",
    "class CardClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=53):\n",
    "        super(CardClassifier,self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, num_classes)\n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.dropout3 = nn.Dropout(p=0.3)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        output = self.fc4(x)\n",
    "        \n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T03:56:33.702985500Z",
     "start_time": "2025-01-19T03:56:33.698296200Z"
    }
   },
   "id": "ce6a5b1ef38d84b9"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    RandomChoice([\n",
    "        transforms.RandomHorizontalFlip(p=1.0),\n",
    "        transforms.RandomVerticalFlip(p=1.0),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),\n",
    "        transforms.RandomRotation(degrees=30)\n",
    "    ]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)), #Resize to 128x128\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_folder = r'C:\\Users\\piotr\\Desktop\\data\\train'\n",
    "valid_folder = r'C:\\Users\\piotr\\Desktop\\data\\valid'\n",
    "test_folder = r'C:\\Users\\piotr\\Desktop\\data\\test'\n",
    "\n",
    "train_dataset = PlayingCardDataset(train_folder, transform=transform_train)\n",
    "val_dataset = PlayingCardDataset(valid_folder, transform=transform)\n",
    "test_dataset = PlayingCardDataset(test_folder, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T03:56:33.766646300Z",
     "start_time": "2025-01-19T03:56:33.704493100Z"
    }
   },
   "id": "faa6763a2abd6f24"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_model = None\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.status = \"\"\n",
    "\n",
    "    def __call__(self, model, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.status = \"Stopping training\"\n",
    "                if self.restore_best_weights:\n",
    "                    model.load_state_dict(self.best_model)\n",
    "                return True\n",
    "        self.status = f\"{self.counter}/{self.patience}\"\n",
    "        return False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T03:56:33.766646300Z",
     "start_time": "2025-01-19T03:56:33.737704700Z"
    }
   },
   "id": "82a37f15e843ad96"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T03:56:33.768177Z",
     "start_time": "2025-01-19T03:56:33.743915700Z"
    }
   },
   "id": "8328aa1ee6b065d3"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e7c13cfe19a4f5da49559456c1a1417"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e4630285f7f40c482dde8afd4a4e965"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 - Train loss: 3.7699, Validation loss: 2.7505, Train acc: 0.0534, Val acc: 0.1547\n",
      "Early Stopping: 0/5\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f46abb2d9b234f0c84f364345b621227"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "947aad3d728f4aa29fbf2556d167c6c0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200 - Train loss: 2.8062, Validation loss: 1.8718, Train acc: 0.1818, Val acc: 0.3208\n",
      "Early Stopping: 0/5\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f9fa8932f2d4471adc8f87944b630bc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation loop:   0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3479465773824713ab4691fe6ea39bb2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/200 - Train loss: 2.3589, Validation loss: 1.6703, Train acc: 0.2611, Val acc: 0.3396\n",
      "Early Stopping: 0/5\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training loop:   0%|          | 0/239 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78d48255591a4940905856ff97be1448"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[95], line 20\u001B[0m\n\u001B[0;32m     18\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     19\u001B[0m running_loss, running_corrects \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m, \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m tqdm(train_loader, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTraining loop\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;66;03m# Move inputs and labels to the device\u001B[39;00m\n\u001B[0;32m     22\u001B[0m     images, labels \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     24\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\tqdm\\notebook.py:250\u001B[0m, in \u001B[0;36mtqdm_notebook.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    249\u001B[0m     it \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__iter__\u001B[39m()\n\u001B[1;32m--> 250\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m it:\n\u001B[0;32m    251\u001B[0m         \u001B[38;5;66;03m# return super(tqdm...) will not catch exception\u001B[39;00m\n\u001B[0;32m    252\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m    253\u001B[0m \u001B[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\tqdm\\std.py:1181\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1178\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[0;32m   1180\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1181\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[0;32m   1182\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m   1183\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[0;32m   1184\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    707\u001B[0m ):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    756\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 757\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    759\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[90], line 10\u001B[0m, in \u001B[0;36mPlayingCardDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[1;32m---> 10\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[idx]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\datasets\\folder.py:247\u001B[0m, in \u001B[0;36mDatasetFolder.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    245\u001B[0m sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader(path)\n\u001B[0;32m    246\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 247\u001B[0m     sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(sample)\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    249\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m t(img)\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001B[0m, in \u001B[0;36mResize.forward\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m    347\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    348\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    349\u001B[0m \u001B[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    352\u001B[0m \u001B[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001B[39;00m\n\u001B[0;32m    353\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 354\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mresize(img, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msize, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minterpolation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mantialias)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\transforms\\functional.py:477\u001B[0m, in \u001B[0;36mresize\u001B[1;34m(img, size, interpolation, max_size, antialias)\u001B[0m\n\u001B[0;32m    475\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    476\u001B[0m     pil_interpolation \u001B[38;5;241m=\u001B[39m pil_modes_mapping[interpolation]\n\u001B[1;32m--> 477\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F_pil\u001B[38;5;241m.\u001B[39mresize(img, size\u001B[38;5;241m=\u001B[39moutput_size, interpolation\u001B[38;5;241m=\u001B[39mpil_interpolation)\n\u001B[0;32m    479\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F_t\u001B[38;5;241m.\u001B[39mresize(img, size\u001B[38;5;241m=\u001B[39moutput_size, interpolation\u001B[38;5;241m=\u001B[39minterpolation\u001B[38;5;241m.\u001B[39mvalue, antialias\u001B[38;5;241m=\u001B[39mantialias)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001B[0m, in \u001B[0;36mresize\u001B[1;34m(img, size, interpolation)\u001B[0m\n\u001B[0;32m    247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(size, \u001B[38;5;28mlist\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(size) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m):\n\u001B[0;32m    248\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGot inappropriate size arg: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msize\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 250\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img\u001B[38;5;241m.\u001B[39mresize(\u001B[38;5;28mtuple\u001B[39m(size[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]), interpolation)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\PIL\\Image.py:2200\u001B[0m, in \u001B[0;36mImage.resize\u001B[1;34m(self, size, resample, box, reducing_gap)\u001B[0m\n\u001B[0;32m   2192\u001B[0m             \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mreduce(\u001B[38;5;28mself\u001B[39m, factor, box\u001B[38;5;241m=\u001B[39mreduce_box)\n\u001B[0;32m   2193\u001B[0m         box \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2194\u001B[0m             (box[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_x,\n\u001B[0;32m   2195\u001B[0m             (box[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_y,\n\u001B[0;32m   2196\u001B[0m             (box[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_x,\n\u001B[0;32m   2197\u001B[0m             (box[\u001B[38;5;241m3\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_y,\n\u001B[0;32m   2198\u001B[0m         )\n\u001B[1;32m-> 2200\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mim\u001B[38;5;241m.\u001B[39mresize(size, resample, box))\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = epochs_number\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "model = CardClassifier(num_classes=53)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-04)\n",
    "\n",
    "es = EarlyStopping(patience=patience)\n",
    "\n",
    "epoch = 0\n",
    "done = False\n",
    "while epoch < num_epochs and not done:\n",
    "    epoch += 1\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss, running_corrects = 0.0, 0\n",
    "    for images, labels in tqdm(train_loader, desc='Training loop'):\n",
    "        # Move inputs and labels to the device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, preds = torch.max(outputs, 1)  # Pobieranie prognoz\n",
    "        running_corrects += (preds == labels).sum().item()\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_accuracy = running_corrects / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_loss, running_corrects = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc='Validation loop'):\n",
    "            # Move inputs and labels to the device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            _, preds = torch.max(outputs, 1)  # Pobieranie prognoz\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "    val_loss = running_loss / len(val_loader.dataset)\n",
    "    val_accuracy = running_corrects / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{num_epochs} - Train loss: {train_loss:.4f}, Validation loss: {val_loss:.4f}, Train acc: {train_accuracy:.4f}, Val acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    done = es(model, val_loss)\n",
    "    print(f\"Early Stopping: {es.status}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T03:57:11.632409500Z",
     "start_time": "2025-01-19T03:56:33.749834300Z"
    }
   },
   "id": "eaf88ae88bcfb0b4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Visualize Losses\n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss over epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-19T03:57:11.632409500Z"
    }
   },
   "id": "e1c915017fa51726"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(train_accuracies, label='Training accuracy')\n",
    "plt.plot(val_accuracies, label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy over epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-19T03:57:11.633932400Z",
     "start_time": "2025-01-19T03:57:11.633423200Z"
    }
   },
   "id": "96ae70abef540912"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate the Accuracy of the Model using test data\n",
    "\n",
    "model.eval()\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predictions = torch.max(outputs, 1)  # Get the class index with the highest probability\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-19T03:57:11.634444800Z"
    }
   },
   "id": "4970ba310f4cd57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate predictions on test data\n",
    "model.eval()\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predictions = torch.max(outputs, 1)  # Get the class index with the highest probability\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=train_dataset.classes, yticklabels=train_dataset.classes)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-19T03:57:11.635446400Z"
    }
   },
   "id": "117b852df2b13051"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load and preprocess the image\n",
    "def preprocess_image(image_path, transform):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return image, transform(image).unsqueeze(0)\n",
    "\n",
    "# Predict using the model\n",
    "def predict(model, image_tensor, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    return probabilities.cpu().numpy().flatten()\n",
    "\n",
    "# Visualization\n",
    "def visualize_predictions(original_image, probabilities, class_names):\n",
    "    fig, axarr = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "    # Display image\n",
    "    axarr[0].imshow(original_image)\n",
    "    axarr[0].axis(\"off\")\n",
    "\n",
    "    # Display predictions\n",
    "    axarr[1].barh(class_names, probabilities)\n",
    "    axarr[1].set_xlabel(\"Probability\")\n",
    "    axarr[1].set_title(\"Class Predictions\")\n",
    "    axarr[1].set_xlim(0, 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-19T03:57:11.635951500Z"
    }
   },
   "id": "f5015fd501c83feb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = PlayingCardDataset(r'C:\\Users\\piotr\\Desktop\\data\\train')\n",
    "test_image = r'C:\\Users\\piotr\\Desktop\\data\\test\\eight of hearts\\2.jpg'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "original_image, image_tensor = preprocess_image(test_image, transform)\n",
    "probabilities = predict(model, image_tensor, device)\n",
    "\n",
    "\n",
    "class_names = dataset.classes\n",
    "visualize_predictions(original_image, probabilities, class_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-19T03:57:11.637458300Z"
    }
   },
   "id": "cf953d2d196a48bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-19T03:57:11.643604900Z"
    }
   },
   "id": "99b9138071b25433"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
